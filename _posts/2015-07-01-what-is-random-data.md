---
layout: post
title: "What is the Randomness in Data?"
date: 2015-07-01 00:00
tags: data, uncertainty, randomness
---

```lisp
(import [__future__ [division print_function]]
        [random]
        [matplotlib.pylab :as plt]
        [seaborn :as sns]
        [numpy :as np])

%matplotlib inline

(sns.set_palette "deep" :desat .6)
(sns.set_context :rc {"figure.figsize" (, 8 4)})

(def *n* 500)
```

This post is mostly about an itch I wanted to scratch. That itch revolved around two points:

1. The best way to learn how a tool works is under controlled circumstances.
2. "random" really just means "ignorance"

To the first point, I have a feeling that to really learn Data Science we should know how to generate small data sets whose characteristics we know. Then if I want to learn how some technique works, say, box-and-whiskers plot, I will generate lots of data sets with different characteristics and plot them using box-and-whiskers plots (or regression or decision trees). Then--theoretically--when I start working with real data, I'll know have some familiarity with the various kinds of characteristics that can generate that kind of data I'm seeing.

To the second point, it sometimes seems difficult to understand exactly what someone means by "random". Sometimes they just seem to mean "arbitrary" or that bootstraps off some underlying mystical coin flipping process. We have this sense that when I flip a coin, it comes up heads "by chance". And it's not clear exactly what that might mean...despite that fact that, in theory, we could know where the coin started, its characteristics, the force applied to the coin, friction from air, how fast I can catch it and then predict exactly whether it will be heads or tails.

In other cases, random is a synonym for something a bit more pejorative like "noise" or "error". The canonical linear regression equation is composed of a deterministic portion and normally distributed error with mean zero and a constant variance. But "error" makes is seem like we weren't paying attention when we collected the data or perhaps we wrote it down wrong instead of error simply being our ignorance (willfull or otherwise) of the other factors at play in the system we're studying.

I want to advance the strong claim that _in principal_ nothing is random in the sense of "by chance" like there is some underlying reality that rolls dice. I believe that if we had the information, we could predict every roll of the dice. Random just means uncertain.

I hope to assemble a constructive proof of the second point while demonstrating the first.

## Random Y

In order to show that what I'm about to demonstrate does not rely on any particular domain, I'm going to refer to hypothetical measures as simply $ y $, $x\_1$, etc. The outcomes here are not dependent on $y$ being height or miles per gallon or purchases and similarly for $x\_1$, etc.

Let us suppose we collect 500 observations of $y$. One thing we note is that $y$ is never the same for each observation. This characteristic is what makes $y$ a _random variable_: each measurement is different whether of the same entity over time or for different entities. This is unlike a deterministic variable where the value is always the same as if we were all clones born fully formed with the same exact specifications.

Those 500 observations of $y$ are going to be generated by sampling from a random number generator. Specifically they're going to be generated by sampling from a normal distribution with mean of 10.0 and a standard deviation of 0.5. You can see the histogram:


```lisp
(def ys (list (take *n* (repeatedly (fn [] (np.random.normal 10.0 0.5))))))
(plt.hist ys :normed true)
```
![png](/images/2015-07-01_1.png)

## Deterministic Y

What I want to show is that I can generate approximately the same distribution through a deterministic relationship between $y$ and a number of $x\_i$'s. We'll always take $x\_1$ to be known and the rest to be unknown. This will ultimately show that when we look at a single variable and summarize it with a mean or represent it by a distribution, what we're really doing is just ignoring all the factors that influence the variable. And if we didn't, if we knew all of them, the variable would actually be deterministic.

So what we are doing when we look at the univariate representation of $y$ is we're simply ignoring anything that might actually cause or otherwise influence $y$, either directly or indirectly. Perhaps because we're ignorant of it or we're just ignoring it.

To that end, I'm going to start naming those factors. Assume that I have some $x\_1$ that is in the range (-1, 1). Now it doesn't matter what values of $x\_1$ I pick, in this sense, they _are_ arbitrary. I could just pick 500 of them but I'm lazy so I'm going to let the computer do it.  I'm only trying to explain $y$ but not $x\_1$ (or any other $x\_i$). I hope by the end of this discussion to explain why I can do that and why it isn't cheating.

In order to do this, I'm going to write a function that returns an arbitrary value of $x\_i$:


```lisp
(defn arbitrary-x []
  (- (* (random.random) 2.0) 1.0))
```

We start by assuming that $x_1$ is the only factor affecting $y$ and that we do not know what $x_1$ is or, perhaps, that it even exists. The relationship is linear, determined by the following formula:

$$y = 10 + 0.75 x_1$$

Given _this_ relationship, what would a histogram of $y$ look like?


```lisp
(def xs1 (np.array (list (take *n* (repeatedly arbitrary-x)))))
(def ys (+ 10.0 (* 0.75 xs1)))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_2.png)

So it looks like a _uniform_ distribution, more or less (and probably more or less depending on when this notebook is run). This is not unexpected. $x\_1$ is basically uniformly distributed on the range (-1, 1) and there is a simple linear relationship between $y$ and $x\_1$. It's clearly not _normal_ or _Gaussian_.

Now let's assume that we know that $x_1$ determines $y$ and that we also collected that data. Here is the scatter plot:

```lisp
(plt.scatter xs1 ys :c "SteelBlue")
```

![png](/images/2015-07-01_3.png)

That makes sense. We have $x\_1$ and a calculation that leads to $y$.

Let's make our system a bit more complicated and the discussion a wee bit confusing. We're going to differentiate between what the system actually is and what we know. That is, we're going to start adding more $x\_i$ but pretend we don't know what they are or that they exist. We'll simply continue analyzing our system using only $y$ and $x\_1$. We're assuming that we either don't know what the other $x\_i$ are or that we can't measure them.

I'm going to make an additional assumption that the factors are going to be largely independent of each other. Why? Suppose we have the following:

$$ y = 10 + 0.75 x_1 - 0.25 x_2 $$

$$ x\_2 = 0.5 - 0.25 x_1 $$

Then I can substitute $x_1$ for $x_2$ like so:

$$y = 10 + 0.75 x_1 - 0.25(0.5 -0.25 x_1)$$

and simplify:

$$y = 9.875 + 0.8125 x_1$$

In that case, there's no point in adding $x_2$. It rarely happens in real life to that degree but it can happen to _some_ degree and I'm just ruling it out for now.

As before, I'm going to sample values of $x_2$ from a uniform distribution in the range $(-1, 1)$ and simulate the following deterministic equation:

$$y = 10 + 0.75 x_1 - 0.25 x_2$$

```lisp
(def xs2 (np.array (list (take *n* (repeatedly arbitrary-x)))))
(def ys (+ 10.0 (* 0.75 xs1) (* -0.25 xs2)))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_4.png)

That's kind of interesting. It seems to be taking on less of uniform shape.

Let's keep adding new factors to our system. Let's add $x_3$:

$$y = 10 + 0.75 x_1 - 0.25 x_2 + 0.13 x_3$$


```lisp
(def xs3 (np.array (list (take *n* (repeatedly arbitrary-x)))))
(def ys (+ 10.0 (* 0.75 xs1) (* -0.25 xs2) (* 0.13 xs3)))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_5.png)

Now we're starting to see some bunching in the middle. Let's add another factor, a negative one:

$$y = 10 + 0.75 x\_1 - 0.25 x\_2 + 0.13 x\_3 - 0.07 x\_4$$

```lisp
(def xs4 (np.array (list (take *n* (repeatedly arbitrary-x)))))
(def ys (+ 10.0 (* 0.75 xs1) (* -0.25 xs2) (* 0.13 xs3) (* -0.07 xs4)))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_6.png)


And then perhaps a bit more of a positive spin, $x\_5$:

$$y = 10 + 0.75 x\_1 - 0.25 x\_2 + 0.13 x\_3 - 0.07 x\_4 + 0.37 x\_5$$


```lisp
(def xs5 (np.array (list (take *n* (repeatedly arbitrary-x)))))
(def ys (+ 10.0 (* 0.75 xs1) (* -0.25 xs2) (* 0.13 xs3) (* -0.07 xs4) (* 0.37 xs5)))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_7.png)

## Central Limit Theorem

This is looking more and more like a normal distribution and it is. What we're seeing here is predicted by a version of the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). As we add more factors such as $x\_6$, $x\_7$, etc, it should get closer and closer to a Normal Distribution.

But note the relationship between $y$ and all of the $x\_i$s is completely deterministic. The normal distribution is a short hand for explaining $y$ with no other factors. If we had observed all of the $x\_i$'s, we could predict $y$ exactly.

Let's write a function that does this simulation for $m$ values of $x$, called $betas$:


```lisp
(defn simulate [constant betas]
  (let [[generate-xs (fn [] (np.array (list (take *n* (repeatedly arbitrary-x)))))]
        [xs          (list (take (len betas) (repeatedly generate-xs)))]
        [scaled-xs   (reduce (fn [a b] (+ a b)) (np.array (list (map (fn [b x] (* b x)) betas xs))))]]
    (, (+ constant scaled-xs) xs)))
```

Here is the simulator with 8 $x$'s:


```lisp
(def [ys xs] (simulate 10.0 [0.75 -0.25 0.13 -0.07 0.37 0.04 0.21 -0.23]))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_8.png)


If we look at the plot for $y$ and $x\_1$...our "known" factor, we're going to see something like the following:


```lisp
(plt.scatter (get xs 0) ys :c "SteelBlue")
```

![png](/images/2015-07-01_9.png)


This data probably has more pattern than we normally see which we will address in future posts but as a first approximation it demonstrates how noisy data can arise from deterministically combining stochastic variables.

Here is another run of the simulator with 16 $x$'s:


```lisp
(def [ys xs] (simulate 10.0 [0.75 -0.25 0.13 -0.07 0.37 0.04 0.21 -0.23 0.05 -0.03 0.21 -0.03 0.05 -0.12 0.13 0.75]))
(plt.hist ys :normed true)
```

![png](/images/2015-07-01_10.png)


That looks very normally distributed.

Here's the scatter plot for $y$ and $x\_1$:


```lisp
(plt.scatter (get xs 0) ys :c "SteelBlue")
```

![png](/images/2015-07-01_11.png)


This looks very similar to what we might call "noisy" data. Still it has a lot of pattern to it. That will be one of the things we address in future posts.

Our final simulation for this post involves starting with the traditional regression model:

$$\hat{y} = 10.0 + 0.75x\_1 + \epsilon$$

where $\epsilon \sim U(0.0, \sigma)$. We can set $\sigma$ based on our data:


```lisp
(def sigma (np.std ys))

(def noise (np.array (list (take *n* (repeatedly (fn [] (np.random.normal 0.0 sigma)))))))

(def y-hat (+ 10.0 (* 0.75 (get xs 0)) noise))

(plt.scatter (get xs 0) y-hat :c "SteelBlue")
```

![png](/images/2015-07-01_12.png)


## Full Circle

Depending on the run, this is quite close to the fully deterministic model and yet it is generated from a canonical regression model where there is a single deterministic factor and normally distributed "error".

As a start we have shown how a random variable can be the deterministic result of other random variables. We bootstrapped the discussion by taking the explanatory factors as given from a uniform distribution. However, now that the demonstration is complete, we can better explain where the factors come from...their own systems of causes and influences. $y$ is caused by the $x's$ and each $x$ is caused by its own set of factors $w$ and $y$ itself is probably but one factor in some other relation where $y$'s influence $z$.

Now we can expand the our discussion to include the following:

1. The $x$'s were all on the same scale (-1, 1). What happens if they are on different scales?
2. A uniform distribution is not the only distribution that an $x$ can come from...we've just shown that we can generate a normal distribution. Obviously, any $x$ could be normally distributed. I didn't start there on purpose. What other distributions, besides uniform, could we use?
3. We have only quantitative factors as causes and influencers. What if we include qualitative factors?

These will be addressed in future posts.

Cheers.

This post was generated from a iPython notebook using [Hy](https://github.com/hylang/hy) and the [Hy Kernel](https://github.com/bollwyvl/hy_kernel), converted to Markdown using ipython nbconvert and then hand edited.
